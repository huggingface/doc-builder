<script>
import Tip from "../../Tip.svelte";
import Youtube from "../../Youtube.svelte";	
export let fw: "pt" | "tf"
</script>

<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Callbacks

Callbacks are objects that can customize the behavior of the training loop in the PyTorch
[Trainer](main_classes/trainer.html#transformers.Trainer) (this feature is not yet implemented in TensorFlow) that can inspect the training loop
state (for progress reporting, logging on TensorBoard or other ML platforms...) and take decisions (like early
stopping).

Callbacks are "read only" pieces of code, apart from the [TrainerControl](main_classes/callback.html#transformers.TrainerControl) object they return, they
cannot change anything in the training loop. For customizations that require changes in the training loop, you should
subclass [Trainer](main_classes/trainer.html#transformers.Trainer) and override the methods you need (see [trainer](trainer.html) for examples).

By default a [Trainer](main_classes/trainer.html#transformers.Trainer) will use the following callbacks:

- [DefaultFlowCallback](main_classes/callback.html#transformers.DefaultFlowCallback) which handles the default behavior for logging, saving and evaluation.
- [PrinterCallback](main_classes/callback.html#transformers.PrinterCallback) or [ProgressCallback](main_classes/callback.html#transformers.ProgressCallback) to display progress and print the
  logs (the first one is used if you deactivate tqdm through the [TrainingArguments](main_classes/trainer.html#transformers.TrainingArguments), otherwise
  it's the second one).
- [TensorBoardCallback](main_classes/callback.html#transformers.integrations.TensorBoardCallback) if tensorboard is accessible (either through PyTorch >= 1.4
  or tensorboardX).
- [WandbCallback](main_classes/callback.html#transformers.integrations.WandbCallback) if [wandb](https://www.wandb.com/) is installed.
- [CometCallback](main_classes/callback.html#transformers.integrations.CometCallback) if [comet_ml](https://www.comet.ml/site/) is installed.
- [MLflowCallback](main_classes/callback.html#transformers.integrations.MLflowCallback) if [mlflow](https://www.mlflow.org/) is installed.
- [AzureMLCallback](main_classes/callback.html#transformers.integrations.AzureMLCallback) if [azureml-sdk](https://pypi.org/project/azureml-sdk/) is
  installed.

The main class that implements callbacks is [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback). It gets the
[TrainingArguments](main_classes/trainer.html#transformers.TrainingArguments) used to instantiate the [Trainer](main_classes/trainer.html#transformers.Trainer), can access that
Trainer's internal state via [TrainerState](main_classes/callback.html#transformers.TrainerState), and can take some actions on the training loop via
[TrainerControl](main_classes/callback.html#transformers.TrainerControl).


## Available Callbacks

Here is the list of the available [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) in the library:

<a id='transformers.integrations.CometCallback'></a>
> **class transformers.integrations.CometCallback**()


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that sends the logs to [Comet ML](https://www.comet.ml/site/).


<a id='transformers.integrations.CometCallback.setup'></a>
> **setup**(self, args, state, model)


Setup the optional Comet.ml integration.

Environment:
COMET_MODE (`str`, _optional_):
"OFFLINE", "ONLINE", or "DISABLED"
COMET_PROJECT_NAME (`str`, _optional_):
Comet.ml project name for experiments
COMET_OFFLINE_DIRECTORY (`str`, _optional_):
Folder to use for saving offline experiments when `COMET_MODE` is "OFFLINE"

For a number of configurable items in the environment, see [here](https://www.comet.ml/docs/python-sdk/advanced/#comet-configuration-variables).


<a id='transformers.DefaultFlowCallback'></a>
> **class transformers.DefaultFlowCallback**()


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that handles the default flow of the training loop for logs, evaluation
and checkpoints.


<a id='transformers.PrinterCallback'></a>
> **class transformers.PrinterCallback**()


A bare [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that just prints the logs.


<a id='transformers.ProgressCallback'></a>
> **class transformers.ProgressCallback**()


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that displays the progress of training or evaluation.


<a id='transformers.EarlyStoppingCallback'></a>
> **class transformers.EarlyStoppingCallback**(early_stopping_patience: int = 1, early_stopping_threshold: typing.Optional[float] = 0.0)


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that handles early stopping.

> Parameters

- **early_stopping_patience** (`int`) --
  Use with `metric_for_best_model` to stop training when the specified metric worsens for
  `early_stopping_patience` evaluation calls.
- **early_stopping_threshold(`float`,** _optional_) --
  Use with TrainingArguments `metric_for_best_model` and `early_stopping_patience` to denote how
  much the specified metric must improve to satisfy early stopping conditions. `

  This callback depends on [TrainingArguments](main_classes/trainer.html#transformers.TrainingArguments) argument _load_best_model_at_end_ functionality
  to set best_metric in [TrainerState](main_classes/callback.html#transformers.TrainerState).


<a id='transformers.integrations.TensorBoardCallback'></a>
> **class transformers.integrations.TensorBoardCallback**(tb_writer = None)


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that sends the logs to [TensorBoard](https://www.tensorflow.org/tensorboard).

> Parameters

- **tb_writer** (`SummaryWriter`, _optional_) --
  The writer to use. Will instantiate one if not set.


<a id='transformers.integrations.WandbCallback'></a>
> **class transformers.integrations.WandbCallback**()


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that sends the logs to [Weight and Biases](https://www.wandb.com/).


<a id='transformers.integrations.WandbCallback.setup'></a>
> **setup**(self, args, state, model, **kwargs)


Setup the optional Weights & Biases (_wandb_) integration.

One can subclass and override this method to customize the setup if needed. Find more information [here](https://docs.wandb.ai/integrations/huggingface). You can also override the following environment variables:

Environment:
WANDB_LOG_MODEL (`bool`, _optional_, defaults to `False`):
Whether or not to log model as artifact at the end of training. Use along with
_TrainingArguments.load_best_model_at_end_ to upload best model.
WANDB_WATCH (`str`, _optional_ defaults to `"gradients"`):
Can be `"gradients"`, `"all"` or `"false"`. Set to `"false"` to disable gradient
logging or `"all"` to log gradients and parameters.
WANDB_PROJECT (`str`, _optional_, defaults to `"huggingface"`):
Set this to a custom string to store results in a different project.
WANDB_DISABLED (`bool`, _optional_, defaults to `False`):
Whether or not to disable wandb entirely. Set _WANDB_DISABLED=true_ to disable.


<a id='transformers.integrations.MLflowCallback'></a>
> **class transformers.integrations.MLflowCallback**()


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that sends the logs to [MLflow](https://www.mlflow.org/).


<a id='transformers.integrations.MLflowCallback.setup'></a>
> **setup**(self, args, state, model)


Setup the optional MLflow integration.

Environment:
HF_MLFLOW_LOG_ARTIFACTS (`str`, _optional_):
Whether to use MLflow .log_artifact() facility to log artifacts.

This only makes sense if logging to a remote server, e.g. s3 or GCS. If set to _True_ or _1_, will copy
whatever is in [TrainingArguments](main_classes/trainer.html#transformers.TrainingArguments)'s `output_dir` to the local or remote
artifact storage. Using it without a remote storage will just copy the files to your artifact location.


<a id='transformers.integrations.AzureMLCallback'></a>
> **class transformers.integrations.AzureMLCallback**(azureml_run = None)


A [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) that sends the logs to [AzureML](https://pypi.org/project/azureml-sdk/).


## TrainerCallback

<a id='transformers.TrainerCallback'></a>
> **class transformers.TrainerCallback**()


A class for objects that will inspect the state of the training loop at some events and take some decisions. At
each of those events the following arguments are available:

> Parameters

- **args** ([TrainingArguments](main_classes/trainer.html#transformers.TrainingArguments)) --
  The training arguments used to instantiate the [Trainer](main_classes/trainer.html#transformers.Trainer).
- **state** ([TrainerState](main_classes/callback.html#transformers.TrainerState)) --
  The current state of the [Trainer](main_classes/trainer.html#transformers.Trainer).
- **control** ([TrainerControl](main_classes/callback.html#transformers.TrainerControl)) --
  The object that is returned to the [Trainer](main_classes/trainer.html#transformers.Trainer) and can be used to make some decisions.
- **model** ([PreTrainedModel](main_classes/model.html#transformers.PreTrainedModel) or `torch.nn.Module`) --
  The model being trained.
- **tokenizer** ([PreTrainedTokenizer](main_classes/tokenizer.html#transformers.PreTrainedTokenizer)) --
  The tokenizer used for encoding the data.
- **optimizer** (`torch.optim.Optimizer`) --
  The optimizer used for the training steps.
- **lr_scheduler** (`torch.optim.lr_scheduler.LambdaLR`) --
  The scheduler used for setting the learning rate.
- **train_dataloader** (`torch.utils.data.DataLoader`, _optional_) --
  The current dataloader used for training.
- **eval_dataloader** (`torch.utils.data.DataLoader`, _optional_) --
  The current dataloader used for training.
- **metrics** (`Dict[str, float]`) --
  The metrics computed by the last evaluation phase.

  Those are only accessible in the event `on_evaluate`.
- **logs**  (`Dict[str, float]`) --
  The values to log.

  Those are only accessible in the event `on_log`.

  The `control` object is the only one that can be changed by the callback, in which case the event that changes
  it should return the modified version.

  The argument `args`, `state` and `control` are positionals for all events, all the others are
  grouped in `kwargs`. You can unpack the ones you need in the signature of the event using them. As an example,
  see the code of the simple `PrinterCallback`.

> Example:

```python
class PrinterCallback(TrainerCallback):

def on_log(self, args, state, control, logs=None, **kwargs):
_ = logs.pop("total_flos", None)
if state.is_local_process_zero:
print(logs)
```


<a id='transformers.TrainerCallback.on_epoch_begin'></a>
> **on\_epoch\_begin**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the beginning of an epoch.


<a id='transformers.TrainerCallback.on_epoch_end'></a>
> **on\_epoch\_end**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the end of an epoch.


<a id='transformers.TrainerCallback.on_evaluate'></a>
> **on\_evaluate**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called after an evaluation phase.


<a id='transformers.TrainerCallback.on_init_end'></a>
> **on\_init\_end**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the end of the initialization of the [Trainer](main_classes/trainer.html#transformers.Trainer).


<a id='transformers.TrainerCallback.on_log'></a>
> **on\_log**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called after logging the last logs.


<a id='transformers.TrainerCallback.on_prediction_step'></a>
> **on\_prediction\_step**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called after a prediction step.


<a id='transformers.TrainerCallback.on_save'></a>
> **on\_save**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called after a checkpoint save.


<a id='transformers.TrainerCallback.on_step_begin'></a>
> **on\_step\_begin**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the beginning of a training step. If using gradient accumulation, one training step might take
several inputs.


<a id='transformers.TrainerCallback.on_step_end'></a>
> **on\_step\_end**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the end of a training step. If using gradient accumulation, one training step might take
several inputs.


<a id='transformers.TrainerCallback.on_substep_end'></a>
> **on\_substep\_end**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the end of an substep during gradient accumulation.


<a id='transformers.TrainerCallback.on_train_begin'></a>
> **on\_train\_begin**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the beginning of training.


<a id='transformers.TrainerCallback.on_train_end'></a>
> **on\_train\_end**(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs)


Event called at the end of training.


Here is an example of how to register a custom callback with the PyTorch [Trainer](main_classes/trainer.html#transformers.Trainer):

```python
class MyCallback(TrainerCallback):
"A callback that prints a message at the beginning of training"

def on_train_begin(self, args, state, control, **kwargs):
print("Starting training")

trainer = Trainer(
model,
args,
train_dataset=train_dataset,
eval_dataset=eval_dataset,
callbacks=[MyCallback]  # We can either pass the callback class this way or an instance of it (MyCallback())
)
```

Another way to register a callback is to call `trainer.add_callback()` as follows:

```python
trainer = Trainer(...)
trainer.add_callback(MyCallback)
# Alternatively, we can pass an instance of the callback class
trainer.add_callback(MyCallback())
```

## TrainerState

<a id='transformers.TrainerState'></a>
> **class transformers.TrainerState**(epoch: typing.Optional[float] = None, global_step: int = 0, max_steps: int = 0, num_train_epochs: int = 0, total_flos: float = 0, log_history: typing.List[typing.Dict[str, float]] = None, best_metric: typing.Optional[float] = None, best_model_checkpoint: typing.Optional[str] = None, is_local_process_zero: bool = True, is_world_process_zero: bool = True, is_hyper_param_search: bool = False, trial_name: str = None, trial_params: typing.Dict[str, typing.Union[str, float, int, bool]] = None)


A class containing the [Trainer](main_classes/trainer.html#transformers.Trainer) inner state that will be saved along the model and optimizer
when checkpointing and passed to the [TrainerCallback](main_classes/callback.html#transformers.TrainerCallback).

<Tip>

In all this class, one step is to be understood as one update step. When using gradient accumulation, one
update step may require several forward and backward passes: if you use `gradient_accumulation_steps=n`,
then one update step requires going through _n_ batches.

</Tip>

> Parameters

- **epoch** (`float`, _optional_) --
  Only set during training, will represent the epoch the training is at (the decimal part being the
  percentage of the current epoch completed).
- **global_step** (`int`, _optional_, defaults to 0) --
  During training, represents the number of update steps completed.
- **max_steps** (`int`, _optional_, defaults to 0) --
  The number of update steps to do during the current training.
- **total_flos** (`float`, _optional_, defaults to 0) --
  The total number of floating operations done by the model since the beginning of training (stored as floats
  to avoid overflow).
- **log_history** (`List[Dict[str, float]]`, _optional_) --
  The list of logs done since the beginning of training.
- **best_metric** (`float`, _optional_) --
  When tracking the best model, the value of the best metric encountered so far.
- **best_model_checkpoint** (`str`, _optional_) --
  When tracking the best model, the value of the name of the checkpoint for the best model encountered so
  far.
- **is_local_process_zero** (`bool`, _optional_, defaults to `True`) --
  Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on
  several machines) main process.
- **is_world_process_zero** (`bool`, _optional_, defaults to `True`) --
  Whether or not this process is the global main process (when training in a distributed fashion on several
  machines, this is only going to be `True` for one process).
- **is_hyper_param_search** (`bool`, _optional_, defaults to `False`) --
  Whether we are in the process of a hyper parameter search using Trainer.hyperparameter_search. This will
  impact the way data will be logged in TensorBoard.


<a id='transformers.TrainerState.load_from_json'></a>
> **load\_from\_json**(json_path: str)

Create an instance from the content of `json_path`.

<a id='transformers.TrainerState.save_to_json'></a>
> **save\_to\_json**(self, json_path: str)

Save the content of this instance in JSON format inside `json_path`.

## TrainerControl

<a id='transformers.TrainerControl'></a>
> **class transformers.TrainerControl**(should_training_stop: bool = False, should_epoch_stop: bool = False, should_save: bool = False, should_evaluate: bool = False, should_log: bool = False)


A class that handles the [Trainer](main_classes/trainer.html#transformers.Trainer) control flow. This class is used by the
[TrainerCallback](main_classes/callback.html#transformers.TrainerCallback) to activate some switches in the training loop.

> Parameters

- **should_training_stop** (`bool`, _optional_, defaults to `False`) --
  Whether or not the training should be interrupted.

  If `True`, this variable will not be set back to `False`. The training will just stop.
- **should_epoch_stop** (`bool`, _optional_, defaults to `False`) --
  Whether or not the current epoch should be interrupted.

  If `True`, this variable will be set back to `False` at the beginning of the next epoch.
- **should_save** (`bool`, _optional_, defaults to `False`) --
  Whether or not the model should be saved at this step.

  If `True`, this variable will be set back to `False` at the beginning of the next step.
- **should_evaluate** (`bool`, _optional_, defaults to `False`) --
  Whether or not the model should be evaluated at this step.

  If `True`, this variable will be set back to `False` at the beginning of the next step.
- **should_log** (`bool`, _optional_, defaults to `False`) --
  Whether or not the logs should be reported at this step.

  If `True`, this variable will be set back to `False` at the beginning of the next step.

