<script>
import Tip from "../../Tip.svelte";
import Youtube from "../../Youtube.svelte";	
export let fw: "pt" | "tf"
</script>

<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Data Collator

Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of
the same type as the elements of `train_dataset` or `eval_dataset`.

To be able to build batches, data collators may apply some processing (like padding). Some of them (like
[DataCollatorForLanguageModeling](/docs/transformers/master/en/main_classes/data_collator.html#transformers.DataCollatorForLanguageModeling)) also apply some random data augmentation (like random masking)
on the formed batch.

Examples of use can be found in the [example scripts](/docs/transformers/master/en/../examples.html) or [example notebooks](/docs/transformers/master/en/../notebooks.html).


## Default data collator

<a id='transformers.default_data_collator'></a>
> **transformers.default\_data\_collator**(features: typing.List[InputDataClass], return_tensors = pt)


Very simple data collator that simply collates batches of dict-like objects and performs special handling for
potential keys named:

- `label`: handles a single value (int or float) per object
- `label_ids`: handles a list of values per object

Does not do any additional preprocessing: property names of the input object will be used as corresponding inputs
to the model. See glue and ner for example of how it's useful.


## DataCollatorWithPadding

<a id='transformers.DataCollatorWithPadding'></a>
> **class transformers.DataCollatorWithPadding**(tokenizer: PreTrainedTokenizerBase, padding: typing.Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: typing.Optional[int] = None, pad_to_multiple_of: typing.Optional[int] = None, return_tensors: str = pt)


Data collator that will dynamically pad the inputs received.

> Parameters

- **tokenizer** ([PreTrainedTokenizer](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) or [PreTrainedTokenizerFast](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast)) --
  The tokenizer used for encoding the data.
- **padding** (`bool`, `str` or [PaddingStrategy](/docs/transformers/master/en/internal/file_utils.html#transformers.file_utils.PaddingStrategy), _optional_, defaults to `True`) --
  Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
  among:

  - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
    sequence if provided).
  - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the
    maximum acceptable input length for the model if that argument is not provided.
  - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
    different lengths).
- **max_length** (`int`, _optional_) --
  Maximum length of the returned list and optionally padding length (see above).
- **pad_to_multiple_of** (`int`, _optional_) --
  If set will pad the sequence to a multiple of the provided value.

  This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
  7.5 (Volta).


## DataCollatorForTokenClassification

<a id='transformers.DataCollatorForTokenClassification'></a>
> **class transformers.DataCollatorForTokenClassification**(tokenizer: PreTrainedTokenizerBase, padding: typing.Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: typing.Optional[int] = None, pad_to_multiple_of: typing.Optional[int] = None, label_pad_token_id: int = -100, return_tensors: str = pt)


Data collator that will dynamically pad the inputs received, as well as the labels.

> Parameters

- **tokenizer** ([PreTrainedTokenizer](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) or [PreTrainedTokenizerFast](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast)) --
  The tokenizer used for encoding the data.
- **padding** (`bool`, `str` or [PaddingStrategy](/docs/transformers/master/en/internal/file_utils.html#transformers.file_utils.PaddingStrategy), _optional_, defaults to `True`) --
  Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
  among:

  - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
    sequence if provided).
  - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the
    maximum acceptable input length for the model if that argument is not provided.
  - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
    different lengths).
- **max_length** (`int`, _optional_) --
  Maximum length of the returned list and optionally padding length (see above).
- **pad_to_multiple_of** (`int`, _optional_) --
  If set will pad the sequence to a multiple of the provided value.

  This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
  7.5 (Volta).
- **label_pad_token_id** (`int`, _optional_, defaults to -100) --
  The id to use when padding the labels (-100 will be automatically ignore by PyTorch loss functions).


## DataCollatorForSeq2Seq

<a id='transformers.DataCollatorForSeq2Seq'></a>
> **class transformers.DataCollatorForSeq2Seq**(tokenizer: PreTrainedTokenizerBase, model: typing.Optional[typing.Any] = None, padding: typing.Union[bool, str, transformers.file_utils.PaddingStrategy] = True, max_length: typing.Optional[int] = None, pad_to_multiple_of: typing.Optional[int] = None, label_pad_token_id: int = -100, return_tensors: str = pt)


Data collator that will dynamically pad the inputs received, as well as the labels.

> Parameters

- **tokenizer** ([PreTrainedTokenizer](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) or [PreTrainedTokenizerFast](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast)) --
  The tokenizer used for encoding the data.
- **model** ([PreTrainedModel](/docs/transformers/master/en/main_classes/model.html#transformers.PreTrainedModel)) --
  The model that is being trained. If set and has the _prepare_decoder_input_ids_from_labels_, use it to
  prepare the _decoder_input_ids_

  This is useful when using _label_smoothing_ to avoid calculating loss twice.
- **padding** (`bool`, `str` or [PaddingStrategy](/docs/transformers/master/en/internal/file_utils.html#transformers.file_utils.PaddingStrategy), _optional_, defaults to `True`) --
  Select a strategy to pad the returned sequences (according to the model's padding side and padding index)
  among:

  - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single
    sequence is provided).
  - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the
    maximum acceptable input length for the model if that argument is not provided.
  - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of
    different lengths).
- **max_length** (`int`, _optional_) --
  Maximum length of the returned list and optionally padding length (see above).
- **pad_to_multiple_of** (`int`, _optional_) --
  If set will pad the sequence to a multiple of the provided value.

  This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=
  7.5 (Volta).
- **label_pad_token_id** (`int`, _optional_, defaults to -100) --
  The id to use when padding the labels (-100 will be automatically ignored by PyTorch loss functions).


## DataCollatorForLanguageModeling

<a id='transformers.DataCollatorForLanguageModeling'></a>
> **class transformers.DataCollatorForLanguageModeling**(tokenizer: PreTrainedTokenizerBase, mlm: bool = True, mlm_probability: float = 0.15, pad_to_multiple_of: typing.Optional[int] = None, tf_experimental_compile: bool = False, return_tensors: str = pt)


Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they
are not all of the same length.

> Parameters

- **tokenizer** ([PreTrainedTokenizer](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) or [PreTrainedTokenizerFast](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast)) --
  The tokenizer used for encoding the data.
- **mlm** (`bool`, _optional_, defaults to `True`) --
  Whether or not to use masked language modeling. If set to `False`, the labels are the same as the
  inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for
  non-masked tokens and the value to predict for the masked token.
- **mlm_probability** (`float`, _optional_, defaults to 0.15) --
  The probability with which to (randomly) mask tokens in the input, when `mlm` is set to `True`.
- **pad_to_multiple_of** (`int`, _optional_) --
  If set will pad the sequence to a multiple of the provided value.

<Tip>

For best performance, this data collator should be used with a dataset having items that are dictionaries or
BatchEncoding, with the `"special_tokens_mask"` key, as returned by a
[PreTrainedTokenizer](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizer) or a [PreTrainedTokenizerFast](/docs/transformers/master/en/main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast) with the
argument `return_special_tokens_mask=True`.

</Tip>


<a id='transformers.DataCollatorForLanguageModeling.numpy_mask_tokens'></a>
> **numpy\_mask\_tokens**(self, inputs: typing.Any, special_tokens_mask: typing.Optional[typing.Any] = None)


Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.


<a id='transformers.DataCollatorForLanguageModeling.tf_mask_tokens'></a>
> **tf\_mask\_tokens**(self, inputs: typing.Any, vocab_size, mask_token_id, special_tokens_mask: typing.Optional[typing.Any] = None)


Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.


<a id='transformers.DataCollatorForLanguageModeling.torch_mask_tokens'></a>
> **torch\_mask\_tokens**(self, inputs: typing.Any, special_tokens_mask: typing.Optional[typing.Any] = None)


Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.


## DataCollatorForWholeWordMask

<a id='transformers.DataCollatorForWholeWordMask'></a>
> **class transformers.DataCollatorForWholeWordMask**(tokenizer: PreTrainedTokenizerBase, mlm: bool = True, mlm_probability: float = 0.15, pad_to_multiple_of: typing.Optional[int] = None, tf_experimental_compile: bool = False, return_tensors: str = pt)


Data collator used for language modeling that masks entire words.

- collates batches of tensors, honoring their tokenizer's pad_token
- preprocesses batches for masked language modeling

<Tip>

This collator relies on details of the implementation of subword tokenization by
[BertTokenizer](/docs/transformers/master/en/model_doc/bert.html#transformers.BertTokenizer), specifically that subword tokens are prefixed with _##_. For tokenizers
that do not adhere to this scheme, this collator will produce an output that is roughly equivalent to
`.DataCollatorForLanguageModeling`.

</Tip>


<a id='transformers.DataCollatorForWholeWordMask.numpy_mask_tokens'></a>
> **numpy\_mask\_tokens**(self, inputs: typing.Any, mask_labels: typing.Any)


Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set
'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.


<a id='transformers.DataCollatorForWholeWordMask.tf_mask_tokens'></a>
> **tf\_mask\_tokens**(self, inputs: typing.Any, mask_labels: typing.Any)


Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set
'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.


<a id='transformers.DataCollatorForWholeWordMask.torch_mask_tokens'></a>
> **torch\_mask\_tokens**(self, inputs: typing.Any, mask_labels: typing.Any)


Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. Set
'mask_labels' means we use whole word mask (wwm), we directly mask idxs according to it's ref.


## DataCollatorForPermutationLanguageModeling

<a id='transformers.DataCollatorForPermutationLanguageModeling'></a>
> **class transformers.DataCollatorForPermutationLanguageModeling**(tokenizer: PreTrainedTokenizerBase, plm_probability: float = 0.16666666666666666, max_span_length: int = 5, return_tensors: str = pt)


Data collator used for permutation language modeling.

- collates batches of tensors, honoring their tokenizer's pad_token
- preprocesses batches for permutation language modeling with procedures specific to XLNet


<a id='transformers.DataCollatorForPermutationLanguageModeling.numpy_mask_tokens'></a>
> **numpy\_mask\_tokens**(self, inputs: typing.Any)


The masked tokens to be predicted for a particular sequence are determined by the following algorithm:

0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).
1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be
   masked)
2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be
   masked
3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`
4. Set `cur_len = cur_len + context_length`. If `cur_len &amp;lt; max_len` (i.e. there are tokens remaining in
   the sequence to be processed), repeat from Step 1.


<a id='transformers.DataCollatorForPermutationLanguageModeling.tf_mask_tokens'></a>
> **tf\_mask\_tokens**(self, inputs: typing.Any)


The masked tokens to be predicted for a particular sequence are determined by the following algorithm:

0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).
1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be
   masked)
2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be
   masked
3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`
4. Set `cur_len = cur_len + context_length`. If `cur_len &amp;lt; max_len` (i.e. there are tokens remaining in
   the sequence to be processed), repeat from Step 1.


<a id='transformers.DataCollatorForPermutationLanguageModeling.torch_mask_tokens'></a>
> **torch\_mask\_tokens**(self, inputs: typing.Any)


The masked tokens to be predicted for a particular sequence are determined by the following algorithm:

0. Start from the beginning of the sequence by setting `cur_len = 0` (number of tokens processed so far).
1. Sample a `span_length` from the interval `[1, max_span_length]` (length of span of tokens to be
   masked)
2. Reserve a context of length `context_length = span_length / plm_probability` to surround span to be
   masked
3. Sample a starting point `start_index` from the interval `[cur_len, cur_len + context_length - span_length]` and mask tokens `start_index:start_index + span_length`
4. Set `cur_len = cur_len + context_length`. If `cur_len &amp;lt; max_len` (i.e. there are tokens remaining in
   the sequence to be processed), repeat from Step 1.

