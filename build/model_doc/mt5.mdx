<script>
import Tip from "../../Tip.svelte";
import Youtube from "../../Youtube.svelte";	
export let fw: "pt" | "tf"
</script>

<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# mT5

## Overview

The mT5 model was presented in [mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934) by Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya
Siddhant, Aditya Barua, Colin Raffel.

The abstract from the paper is the following:

*The recent "Text-to-Text Transfer Transformer" (T5) leveraged a unified text-to-text format and scale to attain
state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a
multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail
the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual
benchmarks. We also describe a simple technique to prevent "accidental translation" in the zero-shot setting, where a
generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model
checkpoints used in this work are publicly available.*

Note: mT5 was only pre-trained on [mC4](https://huggingface.co/datasets/mc4) excluding any supervised training.
Therefore, this model has to be fine-tuned before it is useable on a downstream task, unlike the original T5 model.
Since mT5 was pre-trained unsupervisedly, there's no real advantage to using a task prefix during single-task
fine-tuning. If you are doing multi-task fine-tuning, you should use a prefix.

Google has released the following variants:

- [google/mt5-small](https://huggingface.co/google/mt5-small)

- [google/mt5-base](https://huggingface.co/google/mt5-base)

- [google/mt5-large](https://huggingface.co/google/mt5-large)

- [google/mt5-xl](https://huggingface.co/google/mt5-xl)

- [google/mt5-xxl](https://huggingface.co/google/mt5-xxl).

This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be
found [here](https://github.com/google-research/multilingual-t5).

## MT5Config

<a id='transformers.MT5Config'></a>
> **class transformers.MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## MT5Tokenizer

<a id='transformers.T5Tokenizer'></a>
> **class transformers.T5Tokenizer**(vocab_file, eos_token = </s>, unk_token = <unk>, pad_token = <pad>, extra_ids = 100, additional_special_tokens = None, sp_model_kwargs: typing.Union[typing.Dict[str, typing.Any], NoneType] = None, **kwargs)


Construct a T5 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).

This tokenizer inherits from [PreTrainedTokenizer](main_classes/tokenizer.html#transformers.PreTrainedTokenizer) which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.

> Parameters

- **vocab_file** (`str`) --
  [SentencePiece](https://github.com/google/sentencepiece) file (generally has a _.spm_ extension) that
  contains the vocabulary necessary to instantiate a tokenizer.
- **eos_token** (`str`, _optional_, defaults to `"&amp;lt;/s>"`) --
  The end of sequence token.

<Tip>

When building a sequence using special tokens, this is not the token that is used for the end of
sequence. The token used is the `sep_token`.

</Tip>

- **unk_token** (`str`, _optional_, defaults to `"&amp;lt;unk>"`) --
  The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
  token instead.
- **pad_token** (`str`, _optional_, defaults to `"&amp;lt;pad>"`) --
  The token used for padding, for example when batching sequences of different lengths.
- **extra_ids** (`int`, _optional_, defaults to 100) --
  Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
  accessible as "&amp;lt;extra_id_&lcub%d}>" where "&lcub%d}" is a number between 0 and extra_ids-1. Extra tokens are
  indexed from the end of the vocabulary up to beginning ("&amp;lt;extra_id_0>" is the last token in the vocabulary
  like in T5 preprocessing see [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)).
- **additional_special_tokens** (`List[str]`, _optional_) --
  Additional special tokens used by the tokenizer.
- **sp_model_kwargs** (`dict`, _optional_) --
  Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things, to set:

  - `enable_sampling`: Enable subword regularization.
  - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.

    - `nbest_size = &lcub0,1}`: No sampling is performed.
    - `nbest_size > 1`: samples from the nbest_size results.
    - `nbest_size &amp;lt; 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
      using forward-filtering-and-backward-sampling algorithm.

  - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
    BPE-dropout.

Attributes:
sp_model (`SentencePieceProcessor`):
The _SentencePiece_ processor that is used for every conversion (string, tokens and IDs).


<a id='transformers.T5Tokenizer.build_inputs_with_special_tokens'></a>
> **build\_inputs\_with\_special\_tokens**(self, token_ids_0: typing.List[int], token_ids_1: typing.Optional[typing.List[int]] = None)


Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:

- single sequence: `X &amp;lt;/s>`
- pair of sequences: `A &amp;lt;/s> B &amp;lt;/s>`

> Parameters

- **token_ids_0** (`List[int]`) --
  List of IDs to which the special tokens will be added.
- **token_ids_1** (`List[int]`, _optional_) --
  Optional second list of IDs for sequence pairs.

> Returns

List of [input IDs](../glossary.html#input-ids) with the appropriate special tokens.

> Return type

`List[int]`


<a id='transformers.T5Tokenizer.convert_tokens_to_string'></a>
> **convert\_tokens\_to\_string**(self, tokens)

Converts a sequence of tokens (string) in a single string.

<a id='transformers.T5Tokenizer.create_token_type_ids_from_sequences'></a>
> **create\_token\_type\_ids\_from\_sequences**(self, token_ids_0: typing.List[int], token_ids_1: typing.Optional[typing.List[int]] = None)


Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.

> Parameters

- **token_ids_0** (`List[int]`) --
  List of IDs.
- **token_ids_1** (`List[int]`, _optional_) --
  Optional second list of IDs for sequence pairs.

> Returns

List of zeros.

> Return type

`List[int]`


<a id='transformers.T5Tokenizer.get_special_tokens_mask'></a>
> **get\_special\_tokens\_mask**(self, token_ids_0: typing.List[int], token_ids_1: typing.Optional[typing.List[int]] = None, already_has_special_tokens: bool = False)


Retrieve sequence ids from a token list that has no special tokens added. This method is called when adding
special tokens using the tokenizer `prepare_for_model` method.

> Parameters

- **token_ids_0** (`List[int]`) --
  List of IDs.
- **token_ids_1** (`List[int]`, _optional_) --
  Optional second list of IDs for sequence pairs.
- **already_has_special_tokens** (`bool`, _optional_, defaults to `False`) --
  Whether or not the token list is already formatted with special tokens for the model.

> Returns

A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.

> Return type

`List[int]`


See [T5Tokenizer](model_doc/t5.html#transformers.T5Tokenizer) for all details.


## MT5TokenizerFast

<a id='transformers.T5TokenizerFast'></a>
> **class transformers.T5TokenizerFast**(vocab_file = None, tokenizer_file = None, eos_token = </s>, unk_token = <unk>, pad_token = <pad>, extra_ids = 100, additional_special_tokens = None, **kwargs)


Construct a "fast" T5 tokenizer (backed by HuggingFace's _tokenizers_ library). Based on [Unigram](https://huggingface.co/docs/tokenizers/python/latest/components.html?highlight=unigram#models).

This tokenizer inherits from [PreTrainedTokenizerFast](main_classes/tokenizer.html#transformers.PreTrainedTokenizerFast) which contains most of the main
methods. Users should refer to this superclass for more information regarding those methods.

> Parameters

- **vocab_file** (`str`) --
  [SentencePiece](https://github.com/google/sentencepiece) file (generally has a _.spm_ extension) that
  contains the vocabulary necessary to instantiate a tokenizer.
- **eos_token** (`str`, _optional_, defaults to `"&amp;lt;/s>"`) --
  The end of sequence token.

<Tip>

When building a sequence using special tokens, this is not the token that is used for the end of
sequence. The token used is the `sep_token`.

</Tip>

- **unk_token** (`str`, _optional_, defaults to `"&amp;lt;unk>"`) --
  The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
  token instead.
- **pad_token** (`str`, _optional_, defaults to `"&amp;lt;pad>"`) --
  The token used for padding, for example when batching sequences of different lengths.
- **extra_ids** (`int`, _optional_, defaults to 100) --
  Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
  accessible as "&amp;lt;extra_id_&lcub%d}>" where "&lcub%d}" is a number between 0 and extra_ids-1. Extra tokens are
  indexed from the end of the vocabulary up to beginning ("&amp;lt;extra_id_0>" is the last token in the vocabulary
  like in T5 preprocessing see [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)).
- **additional_special_tokens** (`List[str]`, _optional_) --
  Additional special tokens used by the tokenizer.


<a id='transformers.T5TokenizerFast.build_inputs_with_special_tokens'></a>
> **build\_inputs\_with\_special\_tokens**(self, token_ids_0: typing.List[int], token_ids_1: typing.Optional[typing.List[int]] = None)


Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and
adding special tokens. A sequence has the following format:

- single sequence: `X &amp;lt;/s>`
- pair of sequences: `A &amp;lt;/s> B &amp;lt;/s>`

> Parameters

- **token_ids_0** (`List[int]`) --
  List of IDs to which the special tokens will be added.
- **token_ids_1** (`List[int]`, _optional_) --
  Optional second list of IDs for sequence pairs.

> Returns

List of [input IDs](../glossary.html#input-ids) with the appropriate special tokens.

> Return type

`List[int]`


<a id='transformers.T5TokenizerFast.create_token_type_ids_from_sequences'></a>
> **create\_token\_type\_ids\_from\_sequences**(self, token_ids_0: typing.List[int], token_ids_1: typing.Optional[typing.List[int]] = None)


Create a mask from the two sequences passed to be used in a sequence-pair classification task. T5 does not make
use of token type ids, therefore a list of zeros is returned.

> Parameters

- **token_ids_0** (`List[int]`) --
  List of IDs.
- **token_ids_1** (`List[int]`, _optional_) --
  Optional second list of IDs for sequence pairs.

> Returns

List of zeros.

> Return type

`List[int]`


<a id='transformers.T5Tokenizer'></a>
> **class T5Tokenizer**(vocab_file, eos_token = </s>, unk_token = <unk>, pad_token = <pad>, extra_ids = 100, additional_special_tokens = None, sp_model_kwargs: typing.Union[typing.Dict[str, typing.Any], NoneType] = None, **kwargs)


Construct a T5 tokenizer. Based on [SentencePiece](https://github.com/google/sentencepiece).

This tokenizer inherits from [PreTrainedTokenizer](main_classes/tokenizer.html#transformers.PreTrainedTokenizer) which contains most of the main methods.
Users should refer to this superclass for more information regarding those methods.

> Parameters

- **vocab_file** (`str`) --
  [SentencePiece](https://github.com/google/sentencepiece) file (generally has a _.spm_ extension) that
  contains the vocabulary necessary to instantiate a tokenizer.
- **eos_token** (`str`, _optional_, defaults to `"&amp;lt;/s>"`) --
  The end of sequence token.

<Tip>

When building a sequence using special tokens, this is not the token that is used for the end of
sequence. The token used is the `sep_token`.

</Tip>

- **unk_token** (`str`, _optional_, defaults to `"&amp;lt;unk>"`) --
  The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this
  token instead.
- **pad_token** (`str`, _optional_, defaults to `"&amp;lt;pad>"`) --
  The token used for padding, for example when batching sequences of different lengths.
- **extra_ids** (`int`, _optional_, defaults to 100) --
  Add a number of extra ids added to the end of the vocabulary for use as sentinels. These tokens are
  accessible as "&amp;lt;extra_id_&lcub%d}>" where "&lcub%d}" is a number between 0 and extra_ids-1. Extra tokens are
  indexed from the end of the vocabulary up to beginning ("&amp;lt;extra_id_0>" is the last token in the vocabulary
  like in T5 preprocessing see [here](https://github.com/google-research/text-to-text-transfer-transformer/blob/9fd7b14a769417be33bc6c850f9598764913c833/t5/data/preprocessors.py#L2117)).
- **additional_special_tokens** (`List[str]`, _optional_) --
  Additional special tokens used by the tokenizer.
- **sp_model_kwargs** (`dict`, _optional_) --
  Will be passed to the `SentencePieceProcessor.__init__()` method. The [Python wrapper for SentencePiece](https://github.com/google/sentencepiece/tree/master/python) can be used, among other things, to set:

  - `enable_sampling`: Enable subword regularization.
  - `nbest_size`: Sampling parameters for unigram. Invalid for BPE-Dropout.

    - `nbest_size = &lcub0,1}`: No sampling is performed.
    - `nbest_size > 1`: samples from the nbest_size results.
    - `nbest_size &amp;lt; 0`: assuming that nbest_size is infinite and samples from the all hypothesis (lattice)
      using forward-filtering-and-backward-sampling algorithm.

  - `alpha`: Smoothing parameter for unigram sampling, and dropout probability of merge operations for
    BPE-dropout.

Attributes:
sp_model (`SentencePieceProcessor`):
The _SentencePiece_ processor that is used for every conversion (string, tokens and IDs).


See [T5TokenizerFast](model_doc/t5.html#transformers.T5TokenizerFast) for all details.


## MT5Model

<a id='transformers.MT5Model'></a>
> **class transformers.MT5Model**(config: T5Config)


This class overrides [T5Model](model_doc/t5.html#transformers.T5Model). Please check the superclass for the appropriate documentation
alongside usage examples.

> Examples:

```python
>>> from transformers import MT5Model, T5Tokenizer
>>> model = MT5Model.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> summary = "Weiter Verhandlung in Syrien."
>>> inputs = tokenizer(article, return_tensors="pt")
>>> with tokenizer.as_target_tokenizer():
...     labels = tokenizer(summary, return_tensors="pt")

>>> outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
>>> hidden_states = outputs.last_hidden_state
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## MT5ForConditionalGeneration

<a id='transformers.MT5ForConditionalGeneration'></a>
> **class transformers.MT5ForConditionalGeneration**(config)


This class overrides [T5ForConditionalGeneration](model_doc/t5.html#transformers.T5ForConditionalGeneration). Please check the superclass for the
appropriate documentation alongside usage examples.

> Examples:

```python
>>> from transformers import MT5ForConditionalGeneration, T5Tokenizer
>>> model = MT5ForConditionalGeneration.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> summary = "Weiter Verhandlung in Syrien."
>>> inputs = tokenizer(article, return_tensors="pt")
>>> with tokenizer.as_target_tokenizer():
...     labels = tokenizer(summary, return_tensors="pt")

>>> outputs = model(**inputs,labels=labels["input_ids"])
>>> loss = outputs.loss
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## MT5EncoderModel

<a id='transformers.MT5EncoderModel'></a>
> **class transformers.MT5EncoderModel**(config: T5Config)


This class overrides [T5EncoderModel](model_doc/t5.html#transformers.T5EncoderModel). Please check the superclass for the appropriate
documentation alongside usage examples.

> Examples:

```python
>>> from transformers import MT5EncoderModel, T5Tokenizer
>>> model = MT5EncoderModel.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> input_ids = tokenizer(article, return_tensors="pt").input_ids
>>> outputs = model(input_ids)
>>> hidden_state = outputs.last_hidden_state
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## TFMT5Model

<a id='transformers.TFMT5Model'></a>
> **class transformers.TFMT5Model**(*args, **kwargs)


This class overrides [TFT5Model](model_doc/t5.html#transformers.TFT5Model). Please check the superclass for the appropriate
documentation alongside usage examples.

> Examples:

```python
>>> from transformers import TFMT5Model, T5Tokenizer
>>> model = TFMT5Model.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> summary = "Weiter Verhandlung in Syrien."
>>> inputs = tokenizer(article, return_tensors="tf")
>>> with tokenizer.as_target_tokenizer():
...     labels = tokenizer(summary, return_tensors="tf")

>>> outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=labels["input_ids"])
>>> hidden_states = outputs.last_hidden_state
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## TFMT5ForConditionalGeneration

<a id='transformers.TFMT5ForConditionalGeneration'></a>
> **class transformers.TFMT5ForConditionalGeneration**(*args, **kwargs)


This class overrides [TFT5ForConditionalGeneration](model_doc/t5.html#transformers.TFT5ForConditionalGeneration). Please check the superclass for the
appropriate documentation alongside usage examples.

> Examples:

```python
>>> from transformers import TFMT5ForConditionalGeneration, T5Tokenizer
>>> model = TFMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> summary = "Weiter Verhandlung in Syrien."
>>> inputs = tokenizer(article, return_tensors="tf")
>>> with tokenizer.as_target_tokenizer():
...     labels = tokenizer(summary, return_tensors="tf")

>>> outputs = model(**inputs,labels=labels["input_ids"])
>>> loss = outputs.loss
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## TFMT5EncoderModel

<a id='transformers.TFMT5EncoderModel'></a>
> **class transformers.TFMT5EncoderModel**(*args, **kwargs)


This class overrides [TFT5EncoderModel](model_doc/t5.html#transformers.TFT5EncoderModel). Please check the superclass for the appropriate
documentation alongside usage examples.

> Examples:

```python
>>> from transformers import TFMT5EncoderModel, T5Tokenizer
>>> model = TFMT5EncoderModel.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")
>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> input_ids = tokenizer(article, return_tensors="tf").input_ids
>>> outputs = model(input_ids)
>>> hidden_state = outputs.last_hidden_state
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## FlaxMT5Model

<a id='transformers.FlaxMT5Model'></a>
> **class transformers.FlaxMT5Model**(config: T5Config, input_shape: typing.Tuple[int] = (1, 1), seed: int = 0, dtype: dtype = <class 'jax._src.numpy.lax_numpy.float32'>, **kwargs)


This class overrides [FlaxT5Model](model_doc/t5.html#transformers.FlaxT5Model). Please check the superclass for the appropriate
documentation alongside usage examples.

> Examples:

```python
>>> from transformers import FlaxMT5Model, T5Tokenizer

>>> model = FlaxMT5Model.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> summary = "Weiter Verhandlung in Syrien."
>>> inputs = tokenizer(article, return_tensors="np")

>>> with tokenizer.as_target_tokenizer():
...     decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

>>> outputs = model(input_ids=inputs["input_ids"], decoder_input_ids=decoder_input_ids)
>>> hidden_states = outputs.last_hidden_state
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).


## FlaxMT5ForConditionalGeneration

<a id='transformers.FlaxMT5ForConditionalGeneration'></a>
> **class transformers.FlaxMT5ForConditionalGeneration**(config: T5Config, input_shape: typing.Tuple[int] = (1, 1), seed: int = 0, dtype: dtype = <class 'jax._src.numpy.lax_numpy.float32'>, **kwargs)


This class overrides [FlaxT5ForConditionalGeneration](model_doc/t5.html#transformers.FlaxT5ForConditionalGeneration). Please check the superclass for the
appropriate documentation alongside usage examples.

> Examples:

```python
>>> from transformers import FlaxMT5ForConditionalGeneration, T5Tokenizer

>>> model = FlaxMT5ForConditionalGeneration.from_pretrained("google/mt5-small")
>>> tokenizer = T5Tokenizer.from_pretrained("google/mt5-small")

>>> article = "UN Offizier sagt, dass weiter verhandelt werden muss in Syrien."
>>> summary = "Weiter Verhandlung in Syrien."
>>> inputs = tokenizer(article, return_tensors="np")

>>> with tokenizer.as_target_tokenizer():
...     decoder_input_ids = tokenizer(summary, return_tensors="np").input_ids

>>> outputs = model(**inputs, decoder_input_ids=decoder_input_ids)
>>> logits = outputs.logits
```


<a id='transformers.MT5Config'></a>
> **class MT5Config**(vocab_size = 250112, d_model = 512, d_kv = 64, d_ff = 1024, num_layers = 8, num_decoder_layers = None, num_heads = 6, relative_attention_num_buckets = 32, dropout_rate = 0.1, layer_norm_epsilon = 1e-06, initializer_factor = 1.0, feed_forward_proj = gated-gelu, is_encoder_decoder = True, use_cache = True, tokenizer_class = T5Tokenizer, tie_word_embeddings = False, pad_token_id = 0, eos_token_id = 1, decoder_start_token_id = 0, **kwargs)


This is the configuration class to store the configuration of a [MT5Model](model_doc/mt5.html#transformers.MT5Model) or a
[TFMT5Model](model_doc/mt5.html#transformers.TFMT5Model). It is used to instantiate a mT5 model according to the specified arguments,
defining the model architecture. Instantiating a configuration with the defaults will yield a similar configuration
to that of the mT5 [google/mt5-small](https://huggingface.co/google/mt5-small) architecture.

Configuration objects inherit from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) and can be used to control the model
outputs. Read the documentation from [PretrainedConfig](main_classes/configuration.html#transformers.PretrainedConfig) for more information.

Arguments:
vocab_size (`int`, _optional_, defaults to 250112):
Vocabulary size of the T5 model. Defines the number of different tokens that can be represented by the
`inputs_ids` passed when calling [T5Model](model_doc/t5.html#transformers.T5Model) or [TFT5Model](model_doc/t5.html#transformers.TFT5Model).
d_model (`int`, _optional_, defaults to 512):
Size of the encoder layers and the pooler layer.
d_kv (`int`, _optional_, defaults to 64):
Size of the key, query, value projections per attention head. `d_kv` has to be equal to `d_model // num_heads`.
d_ff (`int`, _optional_, defaults to 1024):
Size of the intermediate feed forward layer in each `T5Block`.
num_layers (`int`, _optional_, defaults to 8):
Number of hidden layers in the Transformer encoder.
num_decoder_layers (`int`, _optional_):
Number of hidden layers in the Transformer decoder. Will use the same value as `num_layers` if not
set.
num_heads (`int`, _optional_, defaults to 6):
Number of attention heads for each attention layer in the Transformer encoder.
relative_attention_num_buckets (`int`, _optional_, defaults to 32):
The number of buckets to use for each attention layer.
dropout_rate (`float`, _optional_, defaults to 0.1):
The ratio for all dropout layers.
layer_norm_eps (`float`, _optional_, defaults to 1e-6):
The epsilon used by the layer normalization layers.
initializer_factor (`float`, _optional_, defaults to 1):
A factor for initializing all weight matrices (should be kept to 1, used internally for initialization
testing).
feed_forward_proj (`string`, _optional_, defaults to `"gated-gelu"`):
Type of feed forward layer to be used. Should be one of `"relu"` or `"gated-gelu"`.
use_cache (`bool`, _optional_, defaults to `True`):
Whether or not the model should return the last key/values attentions (not used by all models).

