---
local: model-outputs
sections:
- local: transformers.file_utils.ModelOutput
  title: ModelOutput
- local: transformers.modeling_outputs.BaseModelOutput
  title: BaseModelOutput
- local: transformers.modeling_outputs.BaseModelOutputWithPooling
  title: BaseModelOutputWithPooling
- local: transformers.modeling_outputs.BaseModelOutputWithCrossAttentions
  title: BaseModelOutputWithCrossAttentions
- local: transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions
  title: BaseModelOutputWithPoolingAndCrossAttentions
- local: transformers.modeling_outputs.BaseModelOutputWithPast
  title: BaseModelOutputWithPast
- local: transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions
  title: BaseModelOutputWithPastAndCrossAttentions
- local: transformers.modeling_outputs.Seq2SeqModelOutput
  title: Seq2SeqModelOutput
- local: transformers.modeling_outputs.CausalLMOutput
  title: CausalLMOutput
- local: transformers.modeling_outputs.CausalLMOutputWithCrossAttentions
  title: CausalLMOutputWithCrossAttentions
- local: transformers.modeling_outputs.CausalLMOutputWithPast
  title: CausalLMOutputWithPast
- local: transformers.modeling_outputs.MaskedLMOutput
  title: MaskedLMOutput
- local: transformers.modeling_outputs.Seq2SeqLMOutput
  title: Seq2SeqLMOutput
- local: transformers.modeling_outputs.NextSentencePredictorOutput
  title: NextSentencePredictorOutput
- local: transformers.modeling_outputs.SequenceClassifierOutput
  title: SequenceClassifierOutput
- local: transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput
  title: Seq2SeqSequenceClassifierOutput
- local: transformers.modeling_outputs.MultipleChoiceModelOutput
  title: MultipleChoiceModelOutput
- local: transformers.modeling_outputs.TokenClassifierOutput
  title: TokenClassifierOutput
- local: transformers.modeling_outputs.QuestionAnsweringModelOutput
  title: QuestionAnsweringModelOutput
- local: transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput
  title: Seq2SeqQuestionAnsweringModelOutput
- local: transformers.modeling_tf_outputs.TFBaseModelOutput
  title: TFBaseModelOutput
- local: transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling
  title: TFBaseModelOutputWithPooling
- local: transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions
  title: TFBaseModelOutputWithPoolingAndCrossAttentions
- local: transformers.modeling_tf_outputs.TFBaseModelOutputWithPast
  title: TFBaseModelOutputWithPast
- local: transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions
  title: TFBaseModelOutputWithPastAndCrossAttentions
- local: transformers.modeling_tf_outputs.TFSeq2SeqModelOutput
  title: TFSeq2SeqModelOutput
- local: transformers.modeling_tf_outputs.TFCausalLMOutput
  title: TFCausalLMOutput
- local: transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions
  title: TFCausalLMOutputWithCrossAttentions
- local: transformers.modeling_tf_outputs.TFCausalLMOutputWithPast
  title: TFCausalLMOutputWithPast
- local: transformers.modeling_tf_outputs.TFMaskedLMOutput
  title: TFMaskedLMOutput
- local: transformers.modeling_tf_outputs.TFSeq2SeqLMOutput
  title: TFSeq2SeqLMOutput
- local: transformers.modeling_tf_outputs.TFNextSentencePredictorOutput
  title: TFNextSentencePredictorOutput
- local: transformers.modeling_tf_outputs.TFSequenceClassifierOutput
  title: TFSequenceClassifierOutput
- local: transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput
  title: TFSeq2SeqSequenceClassifierOutput
- local: transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput
  title: TFMultipleChoiceModelOutput
- local: transformers.modeling_tf_outputs.TFTokenClassifierOutput
  title: TFTokenClassifierOutput
- local: transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput
  title: TFQuestionAnsweringModelOutput
- local: transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput
  title: TFSeq2SeqQuestionAnsweringModelOutput
- local: transformers.modeling_flax_outputs.FlaxBaseModelOutput
  title: FlaxBaseModelOutput
- local: transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast
  title: FlaxBaseModelOutputWithPast
- local: transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling
  title: FlaxBaseModelOutputWithPooling
- local: transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions
  title: FlaxBaseModelOutputWithPastAndCrossAttentions
- local: transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput
  title: FlaxSeq2SeqModelOutput
- local: transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions
  title: FlaxCausalLMOutputWithCrossAttentions
- local: transformers.modeling_flax_outputs.FlaxMaskedLMOutput
  title: FlaxMaskedLMOutput
- local: transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput
  title: FlaxSeq2SeqLMOutput
- local: transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput
  title: FlaxNextSentencePredictorOutput
- local: transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput
  title: FlaxSequenceClassifierOutput
- local: transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput
  title: FlaxSeq2SeqSequenceClassifierOutput
- local: transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput
  title: FlaxMultipleChoiceModelOutput
- local: transformers.modeling_flax_outputs.FlaxTokenClassifierOutput
  title: FlaxTokenClassifierOutput
- local: transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput
  title: FlaxQuestionAnsweringModelOutput
- local: transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput
  title: FlaxSeq2SeqQuestionAnsweringModelOutput
title: Model outputs
---
<script>
import Tip from "./Tip.svelte";
import Youtube from "./Youtube.svelte";
import Docstring from "./Docstring.svelte";
import CodeBlock from "./CodeBlock.svelte";
import CodeBlockFw from "./CodeBlockFw.svelte";
import IconCopyLink from "./IconCopyLink.svelte";

export let fw: "pt" | "tf"
</script>

<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

<h1 id="model-outputs">Model outputs</h1>

All models have outputs that are instances of subclasses of [ModelOutput](/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput). Those are
data structures containing all the information returned by the model, but that can also be used as tuples or
dictionaries.

Let's see of this looks on an example:

```
from transformers import BertTokenizer, BertForSequenceClassification
import torch

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1
outputs = model(**inputs, labels=labels)
```

The `outputs` object is a [SequenceClassifierOutput](/docs/transformers/master/en/main_classes/output#transformers.modeling_outputs.SequenceClassifierOutput), as we can see in the
documentation of that class below, it means it has an optional `loss`, a `logits` an optional `hidden_states` and
an optional `attentions` attribute. Here we have the `loss` since we passed along `labels`, but we don't have
`hidden_states` and `attentions` because we didn't pass `output_hidden_states=True` or
`output_attentions=True`.

You can access each attribute as you would usually do, and if that attribute has not been returned by the model, you
will get `None`. Here for instance `outputs.loss` is the loss computed by the model, and `outputs.attentions` is
`None`.

When considering our `outputs` object as tuple, it only considers the attributes that don't have `None` values.
Here for instance, it has two elements, `loss` then `logits`, so

```
outputs[:2]
```

will return the tuple `(outputs.loss, outputs.logits)` for instance.

When considering our `outputs` object as dictionary, it only considers the attributes that don't have `None`
values. Here for instance, it has two keys that are `loss` and `logits`.

We document here the generic model outputs that are used by more than one model type. Specific output types are
documented on their corresponding model page.

<h2 id="transformers.file_utils.ModelOutput">ModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.file\_utils.ModelOutput</name><anchor>transformers.file_utils.ModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/file_utils.py#L2104</source><parameters>""</parameters></docstring>

Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like
a tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular
python dictionary.

<Tip warning={true}>

You can't unpack a `ModelOutput` directly. Use the [to_tuple()](/docs/transformers/master/en/main_classes/output#transformers.file_utils.ModelOutput.to_tuple)
method to convert it to a tuple before.

</Tip>



<div class="docstring">
<docstring><name>to\_tuple</name><anchor>transformers.file_utils.ModelOutput.to_tuple</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/file_utils.py#L2190</source><parameters>[]</parameters></docstring>

Convert self to a tuple containing all the attributes/keys that are not `None`.


</div></div>

<h2 id="transformers.modeling_outputs.BaseModelOutput">BaseModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.BaseModelOutput</name><anchor>transformers.modeling_outputs.BaseModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L24</source><parameters>[{"name": "last_hidden_state", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs, with potential hidden states and attentions.




</div>

<h2 id="transformers.modeling_outputs.BaseModelOutputWithPooling">BaseModelOutputWithPooling</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.BaseModelOutputWithPooling</name><anchor>transformers.modeling_outputs.BaseModelOutputWithPooling</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L50</source><parameters>[{"name": "last_hidden_state", "val": ": FloatTensor = None"}, {"name": "pooler_output", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **pooler_output** (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) --
  Last layer hidden-state of the first token of the sequence (classification token) after further processing
  through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
  the classification token after processing through a linear layer and a tanh activation function. The linear
  layer weights are trained from the next sentence prediction (classification) objective during pretraining.
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that also contains a pooling of the last hidden states.




</div>

<h2 id="transformers.modeling_outputs.BaseModelOutputWithCrossAttentions">BaseModelOutputWithCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.BaseModelOutputWithCrossAttentions</name><anchor>transformers.modeling_outputs.BaseModelOutputWithCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L121</source><parameters>[{"name": "last_hidden_state", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs, with potential hidden states and attentions.




</div>

<h2 id="transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions">BaseModelOutputWithPoolingAndCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.BaseModelOutputWithPoolingAndCrossAttentions</name><anchor>transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L154</source><parameters>[{"name": "last_hidden_state", "val": ": FloatTensor = None"}, {"name": "pooler_output", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **pooler_output** (`torch.FloatTensor` of shape `(batch_size, hidden_size)`) --
  Last layer hidden-state of the first token of the sequence (classification token) after further processing
  through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns
  the classification token after processing through a linear layer and a tanh activation function. The linear
  layer weights are trained from the next sentence prediction (classification) objective during pretraining.
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if
  `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
  `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that also contains a pooling of the last hidden states.




</div>

<h2 id="transformers.modeling_outputs.BaseModelOutputWithPast">BaseModelOutputWithPast</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.BaseModelOutputWithPast</name><anchor>transformers.modeling_outputs.BaseModelOutputWithPast</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L82</source><parameters>[{"name": "last_hidden_state", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if
  `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
  `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).




</div>

<h2 id="transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions">BaseModelOutputWithPastAndCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.BaseModelOutputWithPastAndCrossAttentions</name><anchor>transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L203</source><parameters>[{"name": "last_hidden_state", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if
  `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
  `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).




</div>

<h2 id="transformers.modeling_outputs.Seq2SeqModelOutput">Seq2SeqModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.Seq2SeqModelOutput</name><anchor>transformers.modeling_outputs.Seq2SeqModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L249</source><parameters>[{"name": "last_hidden_state", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the decoder of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.




</div>

<h2 id="transformers.modeling_outputs.CausalLMOutput">CausalLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.CausalLMOutput</name><anchor>transformers.modeling_outputs.CausalLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L310</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Language modeling loss (for next-token prediction).
- **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for causal language model (or autoregressive) outputs.




</div>

<h2 id="transformers.modeling_outputs.CausalLMOutputWithCrossAttentions">CausalLMOutputWithCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.CausalLMOutputWithCrossAttentions</name><anchor>transformers.modeling_outputs.CausalLMOutputWithCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L375</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Language modeling loss (for next-token prediction).
- **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Cross attentions weights after the attention softmax, used to compute the weighted average in the
  cross-attention heads.
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `torch.FloatTensor` tuples of length `config.n_layers`, with each tuple containing the
  cached key, value states of the self-attention and the cross-attention layers if model is used in
  encoder-decoder setting. Only relevant if `config.is_decoder = True`.

  Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for causal language model (or autoregressive) outputs.




</div>

<h2 id="transformers.modeling_outputs.CausalLMOutputWithPast">CausalLMOutputWithPast</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.CausalLMOutputWithPast</name><anchor>transformers.modeling_outputs.CausalLMOutputWithPast</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L339</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Language modeling loss (for next-token prediction).
- **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`)

  Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for causal language model (or autoregressive) outputs.




</div>

<h2 id="transformers.modeling_outputs.MaskedLMOutput">MaskedLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.MaskedLMOutput</name><anchor>transformers.modeling_outputs.MaskedLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L455</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Masked language modeling (MLM) loss.
- **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for masked language models outputs.




</div>

<h2 id="transformers.modeling_outputs.Seq2SeqLMOutput">Seq2SeqLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.Seq2SeqLMOutput</name><anchor>transformers.modeling_outputs.Seq2SeqLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L484</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Language modeling loss.
- **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for sequence-to-sequence language models outputs.




</div>

<h2 id="transformers.modeling_outputs.NextSentencePredictorOutput">NextSentencePredictorOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.NextSentencePredictorOutput</name><anchor>transformers.modeling_outputs.NextSentencePredictorOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L544</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `next_sentence_label` is provided) --
  Next sequence prediction (classification) loss.
- **logits** (`torch.FloatTensor` of shape `(batch_size, 2)`) --
  Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
  before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of models predicting if two sentences are consecutive or not.




</div>

<h2 id="transformers.modeling_outputs.SequenceClassifierOutput">SequenceClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.SequenceClassifierOutput</name><anchor>transformers.modeling_outputs.SequenceClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L574</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Classification (or regression if config.num_labels==1) loss.
- **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) --
  Classification (or regression if config.num_labels==1) scores (before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sentence classification models.




</div>

<h2 id="transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput">Seq2SeqSequenceClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.Seq2SeqSequenceClassifierOutput</name><anchor>transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L603</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `label` is provided) --
  Classification (or regression if config.num_labels==1) loss.
- **logits** (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`) --
  Classification (or regression if config.num_labels==1) scores (before SoftMax).
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sequence-to-sequence sentence classification models.




</div>

<h2 id="transformers.modeling_outputs.MultipleChoiceModelOutput">MultipleChoiceModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.MultipleChoiceModelOutput</name><anchor>transformers.modeling_outputs.MultipleChoiceModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L663</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape _(1,)_, _optional_, returned when `labels` is provided) --
  Classification loss.
- **logits** (`torch.FloatTensor` of shape `(batch_size, num_choices)`) --
  _num_choices_ is the second dimension of the input tensors. (see _input_ids_ above).

  Classification scores (before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of multiple choice models.




</div>

<h2 id="transformers.modeling_outputs.TokenClassifierOutput">TokenClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.TokenClassifierOutput</name><anchor>transformers.modeling_outputs.TokenClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L694</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "logits", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided)  --
  Classification loss.
- **logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`) --
  Classification scores (before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of token classification models.




</div>

<h2 id="transformers.modeling_outputs.QuestionAnsweringModelOutput">QuestionAnsweringModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.QuestionAnsweringModelOutput</name><anchor>transformers.modeling_outputs.QuestionAnsweringModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L723</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "start_logits", "val": ": FloatTensor = None"}, {"name": "end_logits", "val": ": FloatTensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.
- **start_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) --
  Span-start scores (before SoftMax).
- **end_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) --
  Span-end scores (before SoftMax).
- **hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of question answering models.




</div>

<h2 id="transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput">Seq2SeqQuestionAnsweringModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_outputs.Seq2SeqQuestionAnsweringModelOutput</name><anchor>transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_outputs.py#L755</source><parameters>[{"name": "loss", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "start_logits", "val": ": FloatTensor = None"}, {"name": "end_logits", "val": ": FloatTensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[torch.FloatTensor]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[torch.FloatTensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[torch.FloatTensor]] = None"}]</parameters><paramsdesc>- **loss** (`torch.FloatTensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.
- **start_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) --
  Span-start scores (before SoftMax).
- **end_logits** (`torch.FloatTensor` of shape `(batch_size, sequence_length)`) --
  Span-end scores (before SoftMax).
- **past_key_values** (`tuple(tuple(torch.FloatTensor))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors
  of shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer)
  of shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(torch.FloatTensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sequence-to-sequence question answering models.




</div>

<h2 id="transformers.modeling_tf_outputs.TFBaseModelOutput">TFBaseModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFBaseModelOutput</name><anchor>transformers.modeling_tf_outputs.TFBaseModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L24</source><parameters>[{"name": "last_hidden_state", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **hidden_states** (`tuple(tf.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs, with potential hidden states and attentions.




</div>

<h2 id="transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling">TFBaseModelOutputWithPooling</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFBaseModelOutputWithPooling</name><anchor>transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L50</source><parameters>[{"name": "last_hidden_state", "val": ": Tensor = None"}, {"name": "pooler_output", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **pooler_output** (`tf.Tensor` of shape `(batch_size, hidden_size)`) --
  Last layer hidden-state of the first token of the sequence (classification token) further processed by a
  Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
  prediction (classification) objective during pretraining.

  This output is usually *not* a good summary of the semantic content of the input, you're often better with
  averaging or pooling the sequence of hidden-states for the whole input sequence.
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that also contains a pooling of the last hidden states.




</div>

<h2 id="transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions">TFBaseModelOutputWithPoolingAndCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions</name><anchor>transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L84</source><parameters>[{"name": "last_hidden_state", "val": ": Tensor = None"}, {"name": "pooler_output", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **pooler_output** (`tf.Tensor` of shape `(batch_size, hidden_size)`) --
  Last layer hidden-state of the first token of the sequence (classification token) further processed by a
  Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
  prediction (classification) objective during pretraining.

  This output is usually *not* a good summary of the semantic content of the input, you're often better with
  averaging or pooling the sequence of hidden-states for the whole input sequence.
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that also contains a pooling of the last hidden states.




</div>

<h2 id="transformers.modeling_tf_outputs.TFBaseModelOutputWithPast">TFBaseModelOutputWithPast</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFBaseModelOutputWithPast</name><anchor>transformers.modeling_tf_outputs.TFBaseModelOutputWithPast</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L132</source><parameters>[{"name": "last_hidden_state", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).




</div>

<h2 id="transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions">TFBaseModelOutputWithPastAndCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFBaseModelOutputWithPastAndCrossAttentions</name><anchor>transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L201</source><parameters>[{"name": "last_hidden_state", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(tf.FloatTensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).




</div>

<h2 id="transformers.modeling_tf_outputs.TFSeq2SeqModelOutput">TFSeq2SeqModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFSeq2SeqModelOutput</name><anchor>transformers.modeling_tf_outputs.TFSeq2SeqModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L244</source><parameters>[{"name": "last_hidden_state", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the decoder of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
  used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.




</div>

<h2 id="transformers.modeling_tf_outputs.TFCausalLMOutput">TFCausalLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFCausalLMOutput</name><anchor>transformers.modeling_tf_outputs.TFCausalLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L304</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(n,)`, _optional_, where n is the number of non-masked labels, returned when `labels` is provided) --
  Language modeling loss (for next-token prediction).
- **logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for causal language model (or autoregressive) outputs.




</div>

<h2 id="transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions">TFCausalLMOutputWithCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFCausalLMOutputWithCrossAttentions</name><anchor>transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L369</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(n,)`, _optional_, where n is the number of non-masked labels, returned when `labels` is provided) --
  Language modeling loss (for next-token prediction).
- **logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for causal language model (or autoregressive) outputs.




</div>

<h2 id="transformers.modeling_tf_outputs.TFCausalLMOutputWithPast">TFCausalLMOutputWithPast</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFCausalLMOutputWithPast</name><anchor>transformers.modeling_tf_outputs.TFCausalLMOutputWithPast</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L333</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(n,)`, _optional_, where n is the number of non-masked labels, returned when `labels` is provided) --
  Language modeling loss (for next-token prediction).
- **logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for causal language model (or autoregressive) outputs.




</div>

<h2 id="transformers.modeling_tf_outputs.TFMaskedLMOutput">TFMaskedLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFMaskedLMOutput</name><anchor>transformers.modeling_tf_outputs.TFMaskedLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L412</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(n,)`, _optional_, where n is the number of non-masked labels, returned when `labels` is provided) --
  Masked language modeling (MLM) loss.
- **logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for masked language models outputs.




</div>

<h2 id="transformers.modeling_tf_outputs.TFSeq2SeqLMOutput">TFSeq2SeqLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFSeq2SeqLMOutput</name><anchor>transformers.modeling_tf_outputs.TFSeq2SeqLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L441</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(n,)`, _optional_, where n is the number of non-masked labels, returned when `labels` is provided) --
  Language modeling loss.
- **logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
  used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for sequence-to-sequence language models outputs.




</div>

<h2 id="transformers.modeling_tf_outputs.TFNextSentencePredictorOutput">TFNextSentencePredictorOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFNextSentencePredictorOutput</name><anchor>transformers.modeling_tf_outputs.TFNextSentencePredictorOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L500</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(n,)`, _optional_, where n is the number of non-masked labels, returned when `next_sentence_label` is provided) --
  Next sentence prediction loss.
- **logits** (`tf.Tensor` of shape `(batch_size, 2)`) --
  Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
  before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of models predicting if two sentences are consecutive or not.




</div>

<h2 id="transformers.modeling_tf_outputs.TFSequenceClassifierOutput">TFSequenceClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFSequenceClassifierOutput</name><anchor>transformers.modeling_tf_outputs.TFSequenceClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L530</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(batch_size, )`, _optional_, returned when `labels` is provided) --
  Classification (or regression if config.num_labels==1) loss.
- **logits** (`tf.Tensor` of shape `(batch_size, config.num_labels)`) --
  Classification (or regression if config.num_labels==1) scores (before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sentence classification models.




</div>

<h2 id="transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput">TFSeq2SeqSequenceClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFSeq2SeqSequenceClassifierOutput</name><anchor>transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L559</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(1,)`, _optional_, returned when `label` is provided) --
  Classification (or regression if config.num_labels==1) loss.
- **logits** (`tf.Tensor` of shape `(batch_size, config.num_labels)`) --
  Classification (or regression if config.num_labels==1) scores (before SoftMax).
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
  used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **encoder_last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sequence-to-sequence sentence classification models.




</div>

<h2 id="transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput">TFMultipleChoiceModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFMultipleChoiceModelOutput</name><anchor>transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L611</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape _(batch_size, )_, _optional_, returned when `labels` is provided) --
  Classification loss.
- **logits** (`tf.Tensor` of shape `(batch_size, num_choices)`) --
  _num_choices_ is the second dimension of the input tensors. (see _input_ids_ above).

  Classification scores (before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of multiple choice models.




</div>

<h2 id="transformers.modeling_tf_outputs.TFTokenClassifierOutput">TFTokenClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFTokenClassifierOutput</name><anchor>transformers.modeling_tf_outputs.TFTokenClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L642</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "logits", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(n,)`, _optional_, where n is the number of unmasked labels, returned when `labels` is provided)  --
  Classification loss.
- **logits** (`tf.Tensor` of shape `(batch_size, sequence_length, config.num_labels)`) --
  Classification scores (before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of token classification models.




</div>

<h2 id="transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput">TFQuestionAnsweringModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFQuestionAnsweringModelOutput</name><anchor>transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L671</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "start_logits", "val": ": Tensor = None"}, {"name": "end_logits", "val": ": Tensor = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(batch_size, )`, _optional_, returned when `start_positions` and `end_positions` are provided) --
  Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.
- **start_logits** (`tf.Tensor` of shape `(batch_size, sequence_length)`) --
  Span-start scores (before SoftMax).
- **end_logits** (`tf.Tensor` of shape `(batch_size, sequence_length)`) --
  Span-end scores (before SoftMax).
- **hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of question answering models.




</div>

<h2 id="transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput">TFSeq2SeqQuestionAnsweringModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_tf\_outputs.TFSeq2SeqQuestionAnsweringModelOutput</name><anchor>transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_tf_outputs.py#L703</source><parameters>[{"name": "loss", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "start_logits", "val": ": Tensor = None"}, {"name": "end_logits", "val": ": Tensor = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.List[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[tensorflow.python.framework.ops.Tensor] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[tensorflow.python.framework.ops.Tensor]] = None"}]</parameters><paramsdesc>- **loss** (`tf.Tensor` of shape `(1,)`, _optional_, returned when `labels` is provided) --
  Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.
- **start_logits** (`tf.Tensor` of shape `(batch_size, sequence_length)`) --
  Span-start scores (before SoftMax).
- **end_logits** (`tf.Tensor` of shape `(batch_size, sequence_length)`) --
  Span-end scores (before SoftMax).
- **past_key_values** (`List[tf.Tensor]`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  List of `tf.Tensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size, num_heads, sequence_length, embed_size_per_head)`).

  Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be
  used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **encoder_last_hidden_state** (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(tf.Tensor)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `tf.Tensor` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(tf.Tensor)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `tf.Tensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sequence-to-sequence question answering models.




</div>

<h2 id="transformers.modeling_flax_outputs.FlaxBaseModelOutput">FlaxBaseModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxBaseModelOutput</name><anchor>transformers.modeling_flax_outputs.FlaxBaseModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L23</source><parameters>[{"name": "last_hidden_state", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs, with potential hidden states and attentions.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast">FlaxBaseModelOutputWithPast</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxBaseModelOutputWithPast</name><anchor>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L49</source><parameters>[{"name": "last_hidden_state", "val": ": ndarray = None"}, {"name": "past_key_values", "val": ": typing.Union[typing.Dict[str, jax._src.numpy.lax_numpy.ndarray], NoneType] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **past_key_values** (`Dict[str, jnp.ndarray]`) --
  Dictionary of pre-computed hidden-states (key and values in the attention blocks) that can be used for fast
  auto-regressive decoding. Pre-computed key and value hidden-states are of shape _[batch_size, max_length]_.
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs, with potential hidden states and attentions.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling">FlaxBaseModelOutputWithPooling</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxBaseModelOutputWithPooling</name><anchor>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L79</source><parameters>[{"name": "last_hidden_state", "val": ": ndarray = None"}, {"name": "pooler_output", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.
- **pooler_output** (`jnp.ndarray` of shape `(batch_size, hidden_size)`) --
  Last layer hidden-state of the first token of the sequence (classification token) further processed by a
  Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence
  prediction (classification) objective during pretraining.
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that also contains a pooling of the last hidden states.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions">FlaxBaseModelOutputWithPastAndCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions</name><anchor>transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L110</source><parameters>[{"name": "last_hidden_state", "val": ": ndarray = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`tuple(tuple(jnp.ndarray))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of
  shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if
  `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if
  `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput">FlaxSeq2SeqModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxSeq2SeqModelOutput</name><anchor>transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L156</source><parameters>[{"name": "last_hidden_state", "val": ": ndarray = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`) --
  Sequence of hidden-states at the output of the last layer of the decoder of the model.

  If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1, hidden_size)` is output.
- **past_key_values** (`tuple(tuple(jnp.ndarray))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of
  shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential
decoding.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions">FlaxCausalLMOutputWithCrossAttentions</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxCausalLMOutputWithCrossAttentions</name><anchor>transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L217</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]]] = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.
- **cross_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Cross attentions weights after the attention softmax, used to compute the weighted average in the
  cross-attention heads.
- **past_key_values** (`tuple(tuple(jnp.ndarray))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `jnp.ndarray` tuples of length `config.n_layers`, with each tuple containing the cached
  key, value states of the self-attention and the cross-attention layers if model is used in encoder-decoder
  setting. Only relevant if `config.is_decoder = True`.

  Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see
  `past_key_values` input) to speed up sequential decoding.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for causal language model (or autoregressive) outputs.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxMaskedLMOutput">FlaxMaskedLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxMaskedLMOutput</name><anchor>transformers.modeling_flax_outputs.FlaxMaskedLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L258</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for masked language models outputs.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput">FlaxSeq2SeqLMOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxSeq2SeqLMOutput</name><anchor>transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L287</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, sequence_length, config.vocab_size)`) --
  Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).
- **past_key_values** (`tuple(tuple(jnp.ndarray))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of
  shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for sequence-to-sequence language models outputs.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput">FlaxNextSentencePredictorOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxNextSentencePredictorOutput</name><anchor>transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L344</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, 2)`) --
  Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation
  before SoftMax).
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of models predicting if two sentences are consecutive or not.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput">FlaxSequenceClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxSequenceClassifierOutput</name><anchor>transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L371</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, config.num_labels)`) --
  Classification (or regression if config.num_labels==1) scores (before SoftMax).
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sentence classification models.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput">FlaxSeq2SeqSequenceClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxSeq2SeqSequenceClassifierOutput</name><anchor>transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L397</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, config.num_labels)`) --
  Classification (or regression if config.num_labels==1) scores (before SoftMax).
- **past_key_values** (`tuple(tuple(jnp.ndarray))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of
  shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sequence-to-sequence sentence classification models.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput">FlaxMultipleChoiceModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxMultipleChoiceModelOutput</name><anchor>transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L454</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, num_choices)`) --
  _num_choices_ is the second dimension of the input tensors. (see _input_ids_ above).

  Classification scores (before SoftMax).
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of multiple choice models.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxTokenClassifierOutput">FlaxTokenClassifierOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxTokenClassifierOutput</name><anchor>transformers.modeling_flax_outputs.FlaxTokenClassifierOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L482</source><parameters>[{"name": "logits", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **logits** (`jnp.ndarray` of shape `(batch_size, sequence_length, config.num_labels)`) --
  Classification scores (before SoftMax).
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of token classification models.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput">FlaxQuestionAnsweringModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxQuestionAnsweringModelOutput</name><anchor>transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L508</source><parameters>[{"name": "start_logits", "val": ": ndarray = None"}, {"name": "end_logits", "val": ": ndarray = None"}, {"name": "hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **start_logits** (`jnp.ndarray` of shape `(batch_size, sequence_length)`) --
  Span-start scores (before SoftMax).
- **end_logits** (`jnp.ndarray` of shape `(batch_size, sequence_length)`) --
  Span-end scores (before SoftMax).
- **hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the model at the output of each layer plus the initial embedding outputs.
- **attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights after the attention softmax, used to compute the weighted average in the self-attention
  heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of question answering models.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>

<h2 id="transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput">FlaxSeq2SeqQuestionAnsweringModelOutput</h2>

<div class="docstring">

<docstring><name>class transformers.modeling\_flax\_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput</name><anchor>transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput</anchor><source>https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_flax_outputs.py#L537</source><parameters>[{"name": "start_logits", "val": ": ndarray = None"}, {"name": "end_logits", "val": ": ndarray = None"}, {"name": "past_key_values", "val": ": typing.Optional[typing.Tuple[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]]] = None"}, {"name": "decoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "decoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "cross_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_last_hidden_state", "val": ": typing.Optional[jax._src.numpy.lax_numpy.ndarray] = None"}, {"name": "encoder_hidden_states", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}, {"name": "encoder_attentions", "val": ": typing.Optional[typing.Tuple[jax._src.numpy.lax_numpy.ndarray]] = None"}]</parameters><paramsdesc>- **start_logits** (`jnp.ndarray` of shape `(batch_size, sequence_length)`) --
  Span-start scores (before SoftMax).
- **end_logits** (`jnp.ndarray` of shape `(batch_size, sequence_length)`) --
  Span-end scores (before SoftMax).
- **past_key_values** (`tuple(tuple(jnp.ndarray))`, _optional_, returned when `use_cache=True` is passed or when `config.use_cache=True`) --
  Tuple of `tuple(jnp.ndarray)` of length `config.n_layers`, with each tuple having 2 tensors of
  shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of
  shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.

  Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention
  blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.
- **decoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.
- **decoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.
- **cross_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the
  weighted average in the cross-attention heads.
- **encoder_last_hidden_state** (`jnp.ndarray` of shape `(batch_size, sequence_length, hidden_size)`, _optional_) --
  Sequence of hidden-states at the output of the last layer of the encoder of the model.
- **encoder_hidden_states** (`tuple(jnp.ndarray)`, _optional_, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`) --
  Tuple of `jnp.ndarray` (one for the output of the embeddings + one for the output of each layer) of
  shape `(batch_size, sequence_length, hidden_size)`.

  Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.
- **encoder_attentions** (`tuple(jnp.ndarray)`, _optional_, returned when `output_attentions=True` is passed or when `config.output_attentions=True`) --
  Tuple of `jnp.ndarray` (one for each layer) of shape `(batch_size, num_heads, sequence_length, sequence_length)`.

  Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the
  self-attention heads.</paramsdesc><paramgroups>0</paramgroups></docstring>

Base class for outputs of sequence-to-sequence question answering models.





<div class="docstring">
<docstring><name>replace</name><anchor>None</anchor><source>https://github.com/huggingface/transformers/blob/master/src/flax/struct.py#L120</source><parameters>[{"name": "**updates", "val": ""}]</parameters></docstring>
"Returns a new object replacing the specified fields with new values.

</div></div>
